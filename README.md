# RNA Sequence Analysis for Non-Model Organism

This repository is a usable, publicly available RNA-Sequence analysis of a non-model organism tutorial. The objective of this tutorial is beyond familiarizing you with how to use bioinformatics tools. By the end of this tutorial you should be well-versed in using bioinformatics tools and taking computational approaches to common biological problems.
All steps have been provided for the UConn CBC Xanadu cluster here with appropriate headers for the Slurm scheduler that can be modified simply to run.  Commands should never be executed on the submit nodes of any HPC machine.  If working on the Xanadu cluster, you should use sbatch scriptname after modifying the script for each stage.  Basic editing of all scripts can be performed on the server with tools, such as nano, vim, or emacs.  If you are new to Linux, please use <a href="https://bioinformatics.uconn.edu/unix-basics/">this</a> handy guide for the operating system commands.  In this guide, you will be working with common bioinformatic file formats, such as <a href="https://en.wikipedia.org/wiki/FASTA_format">FASTA</a>, <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ</a>, <a href="https://en.wikipedia.org/wiki/SAM_(file_format)">SAM/BAM</a>, and <a href="https://en.wikipedia.org/wiki/General_feature_format">GFF3/GTF</a>. You can learn even more about each file format <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/file-formats-tutorial/">here</a>. If you do not have a Xanadu account and are an affiliate of UConn/UCHC, please apply for one <a href="https://bioinformatics.uconn.edu/contact-us/">here</a>.
	
<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
  <li><a href="#First_Point_Header">1 Overview</a></li>
<li><a href="#Second_Point_Header">2 Familiarizing yourself with the raw reads</a></li>
<li><a href="#Third_Point_Header">3 Quality control using sickle</a></li>
<li><a href="#Fourth_Point_Header">4 Assembling the transcriptome using RNA-Seq reads and Trinity</a></li>
<li><a href="#Fifth_Point_Header">5 Identifying coding regions using transdecoder</a></li>
<li><a href="#Sixth_Point_Header">6 Creating an index for the assembled transcriptome using bowtie2</a></li>
<li><a href="#Seventh_Point_Header>7 Aligning reads to the assembled transcriptome using bowtie2</a></li>
 <li><a href="#Eighth_Point_Header">8 Generating counts for differential expression analysis</a></li>
 <li><a href="#Ninth_Point_Header">9 Differential expression analysis using gfold</a></li>
 <li><a href="#Tenth_Point_Header">10 Final steps</a></li>
</ul>
</div>

<h2 id="First_Point_Header">Overview</h2>

In this tutorial we will be analyzing RNA-Sequence data from pine needle samples of the red spruce. This data is not published and therefore can only be accessed through the Xanadu directory in "/UCHC/PublicShare/RNASeq_Workshop/". We will be using the red spruce as a "non-model" organism. You may be quite familiar with "model" organisms, such as <i>Drosophila melanogaster</i>, <i>Caenorhabditis elegans</i>, or <i>Arabidopsis thaliana</i>. What is it that makes an organism a "model"? A few things. But most critically, the ease with which the organisms may be grown in a controlled setting, how quick the organisms mature and may reproduce, the cost of growing and maintaining a population of an organism, and the ease with which the organism genomics may be manipulated manually. From this we see why the organisms listed prior are good models -- nemotodes, fruit flies, and weeds are small so do not take much space and may be grown in a laboratory, they reach maturity and reproduce at a very early age (in some instances at a week old!), are quite robust in needing few nutrients to survive, and lastly can be genetically modified quite easily (whether through artificial selection, transposable elements, or chemical mutagenesis). From all of this we can see why the <a href="https://en.wikipedia.org/wiki/Picea_rubens">red spruce</a> is not a great model! 

When an organism is called "model" there is an underlying assumption that very generous amounts of research have been performed on the species resulting in large pools of publicly available data. In biology and bioinformatics this means there are reference transcriptomes, structural annotations, known variant genotypes, and a wealth of other useful information in computational research. By contrast, when an organism is called "non-model" there is the underlying assumption that the information described prior will have to be generated by the research. This means that after extracting genetic data from a non-model organism, the researcher will then have to assemble the transcriptome, annotate the transcriptome, identify any pertinent genetic relationships, and so on. We can use this to develop a small map of our goals for analyzing our red spruce RNA samples. That is:

1. (after quality control) Assemble the transcriptome <br>
2. Remove duplicated regions of the assembled transcriptome through clustering<br>
3. Mapping RNA reads to the assembled transcriptome<br>
4. Generate the counts for the RNA reads<br>
5. Perform differential expression analysis using counts and assembled transcriptome<br>

Our red spruce samples fall into three categories:

1. ambient CO<sub>2</sub> levels
2. elevated CO<sub>2</sub> levels
3. Cotreated for both conditions

Each of our steps are contained in their own subdirectories. After logging into Xanadu, you may find them at:

<pre style="color: silver; background: black;">-bash-4.2$ cd /UCHC/PublicShare/RNASeq_Workshop/RedSpruce/
-bash-4.2$ ls
<b>Align  Assembly  blast2go  Clustered  CodingRegions  Count  gfold  Index  QualityControl</b>  README  <b>Reads</b>  RedSpruce.tar.gz</pre>

Let's begin the analysis now:

<h2 id="Second_Point_Header">Familiarizing yourself with the raw reads</h2>

The reads with which we will be working have been sequenced using <a href="https://www.illumina.com/techniques/sequencing.html">Illumina</a>. We assume that you are familiar with the sequencing technology. Let's have a look at the content of one of our reads, which are in the "fastq" format:

<pre style="color: silver; background: black;">-bash-4.2$ cd Reads/Illumina/
-bash-4.2$ ls
ambient.fastq  cotreated.fastq  elevated.fastq
-bash-4.2$ head ambient.fastq
@HWI-ST318_0118:3:1:1185:2192#0/1
CCCTTCCTGAGATGTAAGTCCTGATCTTGANNNTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
+HWI-ST318_0118:3:1:1185:2192#0/1
cbabc`cY``UVVZT^\\^^b\bbb`Z_BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
@HWI-ST318_0118:3:1:1154:2200#0/1
CACCTGAATGAGCTAAATGACAAATTGCTTNNNANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
+HWI-ST318_0118:3:1:1154:2200#0/1
`ccc^b`bb_`Ybbbdcdddccbcc\aaBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB
@HWI-ST318_0118:3:1:1174:2207#0/1
TTGGCTCACCTCCTCATGTGGTAACTGATGNNNCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
</pre>

We see that for our first three runs we have information about the sampled read including its length followed by the nucleotide read and then a "+" sign. The "+" sign marks the beginning of the corresponding scores for each nucleotide read for the nucleotide sequence preceding the "+" sign. 

<h2 id="Third_Point_Header">Quality control using sickle</h2>

<pre style="color: silver; background: black;">-bash-4.2$ module load sickle

-bash-4.2$ sickle

<strong>Usage</strong>: sickle <command> [options]

<strong>Command</strong>:
pe	paired-end sequence trimming
se	single-end sequence trimming

--help, display this help and exit
--version, output version  Information and exit</pre>

We have single-end sequences. 

<pre style="color: silver; background: black;">-bash-4.2$ sickle se

<strong>Usage</strong>: sickle se [options] -f <fastq sequence file> -t <quality type> -o <trimmed fastq file>

<strong>Options</strong>:
-f, --fastq-file, Input fastq file (required)
-t, --qual-type, Type of quality values (solexa (CASAVA < 1.3), illumina (CASAVA 1.3 to 1.7), sanger (which is CASAVA >= 1.8)) (required)
-o, --output-file, Output trimmed fastq file (required)
-q, --qual-threshold, Threshold for trimming based on average quality in a window. Default 20.
-l, --length-threshold, Threshold to keep a read based on length after trimming. Default 20.
-x, --no-fiveprime, Don't do five prime trimming.
-n, --trunc-n, Truncate sequences at position of first N.
-g, --gzip-output, Output gzipped files.
--quiet, Don't print out any trimming  Information
--help, display this help and exit
--version, output version  Information and exit</pre>

The quality may be any score from 0 to 40. The default of 20 is much too low for a robust analysis. We want to select only reads with a quality of 35 or better. Additionally, the desired length of each read is 50bp. Again, we see that a default of 20 is much too low for analysis confidence. We want to select only reads whose lengths exceed 45bp. Lastly, we must know the scoring type. While the quality type is not listed on the SRA pages, most SRA reads use the "sanger" quality type. Unless explicitly stated, try running sickle using the sanger qualities. If an error is returned, try illumina. If another error is returned, lastly try solexa.

We now write our script with the appropriate <a href="https://bioinformatics.uconn.edu/resources-and-events/tutorials/xanadu/#Xanadu_6">Slurm arguments</a> followed by our commands:

<pre style="color: silver; background: black;">-bash-4.2$ cd ../../QualityControl/Illumina/
-bash-4.2$ ls
ambient.trimmed.fastq  cotreated.trimmed.fastq  elevated.trimmed.fastq  sickle_267830.out  sickle.sh
-bash-4.2 nano sickle.sh
#!/bin/bash
#SBATCH --job-name=sickle_run
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o sickle_%j.out
#SBATCH -e sickle_%j.err

module load sickle

sickle se -f ../../Reads/Illumina/ambient.fastq -t illumina -o ambient.trimmed.fastq -q 30 -l 35
#sickle se -f ../../Reads/Illumina/elevated.fastq -t illumina -o elevated.trimmed.fastq -q 30 -l 35
#sickle se -f ../../Reads/Illumina/cotreated.fastq -t illumina -o cotreated.trimmed.fastq -q 30 -l 35
                                                                                             [ Read 18 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell</pre>


We now press CTRL+X which will ask us if we wish to save, simply type "y" to confirm that we do want to save. Next we will be prompted with the file name, we simply hit enter here to save our file (or, if you like, you may change the file name). Now that we have our script, we may run it with the command:

<pre style="color: silver; background: black;">-bash-4.2$ sbatch sickle.sh</pre>

Notice how the "../../" destination is used to locate the raw reads. "../../" tells the shell to travel back two parent directories, which would be "/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/". Therefore, we see that "../../Reads/Illumina/ambient.fastq" is actually "/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/Reads/Illumina/ambient.fastq". Also notice how there are no parent directories for the output command. This will place the output in the directory in which the script is run, which is "/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/QualityControl/Illumina/ambient.trimmed.fastq".

If we want to see how the quality control has affected our data we can view the standard output file with the following command:

<pre style="color: silver; background: black;">-bash-4.2$ nano sickle*out
  GNU nano 2.3.1                                                     File: sickle_267830.out                                                                                                                


SE input file: ../../Reads/Illumina/ambient.fastq

Total FastQ records: 103531742
FastQ records kept: 83066649
FastQ records discarded: 20465093


SE input file: ../../Reads/Illumina/cotreated.fastq

Total FastQ records: 101490246
FastQ records kept: 81173217
FastQ records discarded: 20317029


SE input file: ../../Reads/Illumina/elevated.fastq

Total FastQ records: 97712226
FastQ records kept: 81595012
FastQ records discarded: 16117214
                                                                                             [ Read 21 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell
</pre>

<h2 id="Fourth_Point_Header">Assembling the transcriptome using RNA-Seq reads and Trinity</h2>

Now that we've performed quality control we are ready to assemble our transcriptome using the RNA-Seq reads. We will be using the software <a href="https://github.com/trinityrnaseq/trinityrnaseq/wiki">Trinity</a>. Nearly all transcriptome assembly software operates under the same premise. Consider the following,

Suppose we have the following three reads:

<pre style="color: silver; background: black;"><b>A C G A C G T T T G A G A</b>
<b>T T G A G A T T A C C T A G</b>
<b? T T A C C T A G A T T G G T G T A</b></pre>

We notice that the end of each read is the beginning of the next read, so we assemble them as one sequence by matching the overlaps:

<pre style="color: silver; background: black;"><b>A C G A C G T [T T G A G A] [T T A C C T A] G A T T G G T G T A </b></pre>

Simple as that!

We will be running Trinity with only the necessary comands. These are:

<pre style="color: silver; background: black;">--seqType		fasta, fastq, etc.
--max-memory		Ideally a value of "250G" or greater
--single		Read-type (could also be --paired)
--min-contig-length	Minimum length assembled sequence must be to make it into the final transcriptome
--CPU			Number of cores
--normailze_reads 	<a href="http://ivory.idyll.org/blog/trinity-in-silico-normalize.html">in silico normalization</a> (recommended for large datasets)</pre>

Knowing this we can <i>assemble</i> our script! The script may be viewed via:

<pre style="color: silver; background: black;">-bash-4.2$ cd ../../Assembly/Trinity/
-bash-4.2$ ls
<b>trinity_out_dir</b>  trinity.sh
-bash-4.2$ nano trinity.sh

#!/bin/bash
#SBATCH --job-name=trinity
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o trinity_%j.out
#SBATCH -e trinity_%j.err

module load Trinity

#Trinity --seqType fq \
        --max_memory 256G \
        --single ../../QualityControl/Illumina/ambient.trimmed.fastq \
        --min_contig_length 300 \
        --CPU 16 \
        --normalize_reads

#Trinity --seqType fq \
        --max_memory 256G \
        --single ../../QualityControl/Illumina/cotreated.trimmed.fastq\
        --min_contig_length 300 \
        --CPU 16 \
        --normalize_reads

#Trinity --seqType fq \
        --max_memory 256G \
        --single ../../QualityControl/Illumina/elevated.trimmed.fastq \
        --min_contig_length 300 \
        --CPU 16 \
        --normalize_reads
                                                                                             [ Read 35 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell</pre>

We use "sbatch" to run the script. Trinity creates an output directory with all of the information determined. Let's have a look inside that directory:

<pre style="color: silver; background: black;">-bash-4.2$ cd trinity_out_dir
-bash-4.2$ ls
chrysalis                        inchworm.kmer_count          jellyfish.kmers.fa.histo         pipeliner.35650.cmds    recursive_trinity.cmds.completed  single.fa.ok
inchworm.K25.L25.DS.fa           insilico_read_normalization  partitioned_reads.files.list     read_partitions         recursive_trinity.cmds.ok         single.fa.read_count
inchworm.K25.L25.DS.fa.finished  jellyfish.kmers.fa           partitioned_reads.files.list.ok  recursive_trinity.cmds  single.fa                         Trinity.fasta
</pre>

We have a lot of files! While we will not be going through them one-by-one using the 'head' command, we will be quickly covering more specifically how Trinity works, per the Trinity github:

<i>Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-seq reads. Trinity partitions the sequence data into many individual de Bruijn graphs, each representing the transcriptional complexity at a given gene or locus, and then processes each graph independently to extract full-length splicing isoforms and to tease apart transcripts derived from paralogous genes. Briefly, the process works like so:

Inchworm assembles the RNA-seq data into the unique sequences of transcripts, often generating full-length transcripts for a dominant isoform, but then reports just the unique portions of alternatively spliced transcripts.

Chrysalis clusters the Inchworm contigs into clusters and constructs complete de Bruijn graphs for each cluster. Each cluster represents the full transcriptonal complexity for a given gene (or sets of genes that share sequences in common). Chrysalis then partitions the full read set among these disjoint graphs.

Butterfly then processes the individual graphs in parallel, tracing the paths that reads and pairs of reads take within the graph, ultimately reporting full-length transcripts for alternatively spliced isoforms, and teasing apart transcripts that corresponds to paralogous genes.</i>

You may notice that there is output for a software called 'jellyfish'. The only file we must worry about is 'Trinity.fasta', which is our final assembled transcriptome.

<h2 id="Fourth_Point_Header">Determining and removing repeat modules in 'Trinity.fasta' by clustering</h2>

Because we used RNA reads to sequence our transcriptome, chances are that there are multiples of the same reads varying slightly which create multiples of the same assembled sequence. Under this assumption, we may also assume that most of the modules in our assembled transcriptome are actually repeats, the results of the assembly of slightly different reads from the same gene. We want to remove the repeats of these modules to shorten the length of our transcriptome and make for more efficient work in the future. We can do this by partitioning and clustering the transcriptome, then taking only one module from each of the clusters. There is a very convenient software which performs all of this for us in the exact way just described: <a href="https://github.com/torognes/vsearch">vsearch</a>. 

vsearch has the following options:

<pre style="color: silver; background: black;">--threads		Number of cores 
--log 			File to log progress
--cluster_fast		Fasta file to cluster
--id			Percent similarity needed to claim redundancy (0.00-1.00)
--centroids		Output fasta file
--uc			Output cluster information file</pre>

Let's have a look at our vsearch script:

<pre style="color: silver; background: black;">-bash-4.2$ cd ../../../Clustered/
-bash-4.2$ ls
centroids.fasta  clusters.uc  cluster_269085.err  cluster.sh  LOGFile
-bash-4.2$ nano cluster.sh
  GNU nano 2.3.1                                                       File: combine.sh                                                                                                                     

#!/bin/bash
#SBATCH --job-name=combine
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o combine_%j.out
#SBATCH -e combine_%j.err

module load vsearch

vsearch --threads 8 --log LOGFile \
        --cluster_fast ../../Assembly/Trinity/trinity_output_dir/Trinity.fasta \
        --id 0.80 --centroids centroids.fasta --uc clusters.uc
                                                                                             [ Read 21 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell
</pre>

</pre>

We submit our script with the 'sbatch' command. 

<h2 id="Fifth_Point_Header">Identifying coding regions using transdecoder</h2>
Now that we have our reads assembled and clustered together into the single centroids file, we can use <a href="https://github.com/TransDecoder/TransDecoder/wiki">TransDecoder</a> to determine optimal open reading frames from the assembly (ORFs). Assembled RNA-Seq transcripts may have 5′ or 3′ UTR sequence attached and this can make it difficult to determine the CDS in non-model species. We will not be going into how TransDecoder works. However, should you click the link you'll be happy to see that they have a very simple one paragraph explanation telling you exactly that.

Our first step is to determine all <a href="https://en.wikipedia.org/wiki/Open_reading_frame">open-reading-frames</a>. We can do this using the 'TransDecoder.LongOrfs' command. This command is quite simple, with one option, '-t', which is simply our centroid fasta! The command is therefore:

<pre style="color: silver; background: black;">TransDecoder.LongOrfs -t ../Clustered/centroids.fasta</pre>

By default it will identify ORFs that are at least 100 amino acids long. (you can change this by using -m parameter). It will produce a folder called centroids.fasta.transdecoder_dir, and will include all the temporary files in that directory. Let's have a look in this directory:

<pre style="color: silver; background: black;">-bash-4.2$ cd ../CodingRegions/
-bash-4.2$ ls
centroids.fasta.transdecoder.bed  <b>centroids.fasta.transdecoder_dir</b>   centroids.fasta.transdecoder.pep  TransDecoder_269848.err  TransDecoder.sh
centroids.fasta.transdecoder.cds  centroids.fasta.transdecoder.gff3  pfam.domtblout   
-bash-4.2$ cd centroids.fasta.transdecoder_dir
-bash-4.2$ ls
centroids.fasta.transdecoder_dir/
longest_orfs.pep
longest_orfs.gff3
longest_orfs.cds
base_freqs.dat.ok
base_freqs.dat</pre>

The file names are quite self-explanatory, and we will live the thinking on these up to you.

Next step is to, identify ORFs with homology to known proteins via blast or <a href="https://pfam.xfam.org/">pfam</a> searches. This will maximize the sensitivity for capturing the ORFs that have functional significance. We will be using the Pfram databases. Pfam stands for "Protein families", and is simply an absolutely massive database with mountains of searchable information on, well, you guessed it, protein families. We can <i>scan</i> the Pfam databases using the software <a href="http://hmmer.org/">hmmer</a>, a database homologous-sequence fetcher. The Pfam databases are much too large to install on a local computer. However, you may find them on Xanadu in the directory '/isg/shared/databases/Pfam/Pfam-B.hmm', which is an hmmer file (must be an hmmer file for hmmer to scan!).

We scan the database using the 'hmmscan' command, which has the following options:
<pre style="color: silver; background: black;">module load hmmer
hmmscan \
--cpu				Number of cores 
--domtblout			Output file for <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2788356/">domain architecture comparisons</a>, this is the output you will be using 
/isg/shared/databases/Pfam/Pfam-B.hmm 	No flag necessary, Pfam hmmer file
centroids.fasta.transdecoder_dir/longest_orfs.pep	No flag necessary, hypothetical amino acid sequence of ORFs</pre>

It is absolutely vital that you place these arguments in the order in which they appear above. You do not want 'hmmscan' thinking your centroids are your database and your database are your centroids!

Our final command will look something like this:

<pre style="color: silver; background: black;">hmmscan --cpu 8 --domtblout pfam.domtblout /common/Pfam/Pfam-B.hmm centroids.fasta.transdecoder_dir/longest_orfs.pep</pre>

Lastly we use the 'TransDecoder.Predict' function to predict the coding regions we should expect in our transcriptome using the output from hmmscan (the pfam.domtblout file).

<pre style="color: silver; background: black;">TransDecoder.Predict -t ../Clustered/centroids.fasta --retain_pfam_hits pfam.domtblout --cpu 8</pre>
 
 This will add output to our 'centroids.fasta.transdecoder_dir' directory. Which now looks like:
 
 <pre style="color: silver; background: black;">-bash-4.2$ ls
 base_freqs.dat     hexamer.scores.ok                      longest_orfs.cds.eclipsed_removed.gff3  longest_orfs.cds.scores.selected     longest_orfs.cds.top_longest_5000             longest_orfs.gff3
base_freqs.dat.ok  longest_orfs.cds                       longest_orfs.cds.scores                 longest_orfs.cds.top_500_longest     longest_orfs.cds.top_longest_5000.nr80        longest_orfs.gff3.inx
hexamer.scores     longest_orfs.cds.best_candidates.gff3  longest_orfs.cds.scores.ok              longest_orfs.cds.top_500_longest.ok  longest_orfs.cds.top_longest_5000.nr80.clstr  longest_orfs.pep
</pre>

Yet again, these filenames are quite self-explanatory and we will leave the thinking up to you. You may view the full script with the command:

 <pre style="color: silver; background: black;">-bash-4.2$ nano ../TransDecoder.sh
   GNU nano 2.3.1                                                    File: ../TransDecoder.sh                                                                                                                

#!/bin/bash
#SBATCH --job-name=TransDecoder
#SBATCH -N 1
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --partition=general
#SBATCH --mail-type=END
#SBATCH --mail-user=your_email@uconn.edu
#SBATCH --mem=50G
#SBATCH -o TransDecoder_%j.out
#SBATCH -e TransDecoder_%j.err

module load hmmer
module load TransDecoder

TransDecoder.LongOrfs -t ../Clustered/centroids.fasta

hmmscan --cpu 8 \
        --domtblout pfam.domtblout /isg/shared/databases/Pfam/Pfam-B.hmm centroids.fasta.transdecoder_dir/longest_orfs.pep

TransDecoder.Predict -t ../Clustered/centroids.fasta --retain_pfam_hits pfam.domtblout --cpu 8

                                                                                             [ Read 22 lines ]
^G Get Help                       ^O WriteOut                       ^R Read File                      ^Y Prev Page                      ^K Cut Text                       ^C Cur Pos
^X Exit                           ^J Justify                        ^W Where Is                       ^V Next Page                      ^U UnCut Text                     ^T To Spell
</pre>

<h2 id="Sixth_Point_Header">Creating an index for the assembled transcriptome using bowtie2</h2>

It is expected by this point that you are quite familiar with the <a href="https://github.com/wolf-adam-eily/refseq_diffexp_funct_annotation_uconn#Fourth_Point_Header">purpose of indexing a transcriptome</a>. We will be using Bowtie2 to index our assembled transcriptome. We use the 'bowtie2-build' command, which has the following options:

 <pre style="color: silver; background: black;">Usage: bowtie2-build [options] [reference_in] [bt2_index_base]  
main arguments
reference_in            comma-separated list of files with ref sequences
bt2_index_base          base name for the index files to write

Options
--threads          # of threads, By default bowtie2-build is using only one thread. Increasing the number of threads will speed up the index building considerably in most cases.</pre>

Therefore, our command will be:

 <pre style="color: silver; background: black;">bowtie2-build ../CodingRegions/centroids.fasta.transdecoder.cds centroids_transdecoder_Index</pre>
 
 The script for this may be found at 'UCHC/PublicShare/RNASeq_Workshop/RedSpruce/Index/index.sh".
 
 <h2 id = "Seventh_Point_Header">Aligning reads to the assembled transcriptome using bowtie2</h2>
 
 Once the index has been created, we will use bowtie2 to align our three trimmed reads. The options for bowtie2 alignent are:
 
  <pre style="color: silver; background: black;">Usage: bowtie2 [Options] -x [bt2_base_index] {reads / options}
Main arguments:
-x [bt2_base_index]     The basename of the index for the reference transcriptome (i.e. with out the *.bt2 extension)
-S                      Output file name of the SAM alignment

Options:
--threads               Number of processors</pre>

Leaving us with a general command of:

<pre style="color: silver; background: black;">bowtie2 --threads 8 -x ../Index/centroids_transdecoder_Index ../QualityControl/Illumina/elevated.trimmed.fastq -S elevated.sam</pre>

You may view the script for alignment at '/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/Align/Align.sh'.

The script will output our files in the SAM format. We can convert these files to BAM with the following general command:

<pre style="color: silver; background: black;">samtools view -uhS elevated.sam | samtools sort -o elevated_sorted</pre>

Which has options:

<pre style="color: silver; background: black;">Usage: samtools [command] [options] in.sam
Command:
view     prints all alignments in the specified input alignment file (in SAM, BAM, or CRAM format) to standard output in SAM format 

Options:
-h      Include the header in the output
-S      Indicate the input was in SAM format
-u      Output uncompressed BAM. This option saves time spent on compression/decompression and is thus preferred when the output is piped to another samtools command


Usage: samtools [command] [-o out.bam]
Command:
sort    Sort alignments by leftmost coordinates

-o      Write the final sorted output to FILE, rather than to standard output.</pre>

You may view the SAM to BAM conversion script at 'sam2bam.sh'.

 <h2 id = "Eighth_Point_Header">Generating counts for differential expression analysis</h2>
 
Once it has been aligned, we will use the <a href="https://pachterlab.github.io/eXpress/overview.html">eXpress</a> software, to get the counts of differential expression.

eXpress is used in the following manner:

<pre style="color: silver; background: black;">Usage:  express [options] [target_seqs.fa] [hits.(sam/bam)]

Required arguments:
 target_seqs.fa     target sequence file in fasta format
 hits.(sam/bam)     read alignment file in SAM or BAM format

Options:
-o [ --output-dir ] arg (=.)        write all output files to this directory</pre>

Giving us a general command of:
<pre style="color: silver; background: black;">express ../CodingRegions2/centroids.fasta.transdecoder.cds ../Align/elevated_sorted.bam -o elevated_express</pre>

You may view the full script at '/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/Count/express.sh'.

 <h2 id = "Ninth_Point_Header">Differential expression analysis using gfold</h2>
Before analyzing for the differential expression using the gfold package, we need to convert our counts to Gfold count format. The program expects, 5 data columns, which are “Gene Symbol”, “Gene Name”, “Read Count”, “Gene exon length”, “FPKM”. To do this we will read the results.xprs file and grab the columns which have the required fields, and will rewrite them to a new file, where it will be used as the input file for the Gfold program.

<b>R</b>
<pre style="color: silver; background: black;">require("plyr")

express_file_1 <- data.frame(read.table("../Count/ambient_express/results.xprs",sep="\t", header = TRUE))
express_file_2 <- data.frame(read.table("../Count/cotreated_express/results.xprs",sep="\t", header = TRUE))
express_file_3 <- data.frame(read.table("../Count/elevated_express/results.xprs",sep="\t", header = TRUE))

subset1 <- data.frame(express_file_1$target_id, express_file_1$target_id, express_file_1$tot_counts, express_file_1$length,express_file_1$fpkm)
subset2 <- data.frame(express_file_2$target_id, express_file_2$target_id, express_file_2$tot_counts, express_file_2$length,express_file_2$fpkm)
subset3 <- data.frame(express_file_3$target_id, express_file_3$target_id, express_file_3$tot_counts, express_file_3$length,express_file_3$fpkm)

sorted1 <- subset1[order(express_file_1$target_id),]
sorted2 <- subset2[order(express_file_2$target_id),]
sorted3 <- subset3[order(express_file_3$target_id),]

subset1 <- rename(subset1,c("express_file_1.target_id"="GeneSymbol","express_file_1.target_id.1"="GeneName","express_file_1.tot_counts"="Read Count","express_file_1.length" ="Gene_exon_length","express_file_1.fpkm"="RPKM"))
subset2 <- rename(subset2,c("express_file_2.target_id"="GeneSymbol","express_file_2.target_id.1"="GeneName","express_file_2.tot_counts"="Read Count","express_file_2.length" ="Gene_exon_length","express_file_2.fpkm"="RPKM"))
subset3 <- rename(subset3,c("express_file_3.target_id"="GeneSymbol","express_file_3.target_id.1"="GeneName","express_file_3.tot_counts"="Read Count","express_file_3.length" ="Gene_exon_length","express_file_3.fpkm"="RPKM"))

write.table(sorted1, "ambient.read_cnt", sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.table(sorted2, "cotreated.read_cnt", sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.table(sorted3, "elevated.read_cnt", sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)</pre>

We have created an R script (eXpress2gfold.R), which will be called, through eXpress2gfold.sh script. The script is located at '/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/gfold/eXpress2gfold.sh'.
The above script will produce the following gfold read count files as the output, which will be used in analyzing the differential expression:

<pre style="color: silver; background: black;">-bash-4.2$ cd /UCHC/PublicShare/RNASeq_Workshop/RedSpruce/gfold/
-bash-4.2$ ls
ambient.read_cnt  cotreated.read_cnt  elevated.read_cnt</pre>

<b>Differential expression using Gfold</b>

Using the formatted count files, we will use Gfold to analyze the differential expression of the three samples. Gfold program is very useful when no replicates are available. Gfold generalizes the fold change by, considering the log fold change value for each gene, and assigns it as a Gfold value. It overcomes the short comping of the p-value, that measure the expression under different conditions, by measuring a relative expression value. It also overcomes the deficiency of low read counts. The following code finds the differentially expressed genes between the two groups of samples.

<pre style="color: silver; background: black;">Usage: gfold [jobs] [options]
Jobs:
diff     Calculate GFOLD value and other statics. It accepts the count values as input. 

Options explained:
-s1       prefix for read_count of 1st group separated by comma.
-s2       prefix for read_count of 2nd group separated by comma.
-suf      suffix of count file
-o        output file and for diff job, there will be two output files, where second will have .ext extension.
-norm     normalization method. 'Count' stands for normalization by total number of mapped reads.</pre>

Giving us the following commands:

<pre style="color: silver; background: black;">gfold diff -s1 cotreated -s2 elevated -suf .read_cnt -o cotreated_VS_elevated.diff
gfold diff -s1 cotreated -s2 ambient -suf .read_cnt -o cotreated_VS_ambient.diff
gfold diff -s1 elevated -s2 ambient -suf .read_cnt -o elevated_VS_ambient.diff</pre>

The prepared script for running Gfold is called, gfold.sh and it is located at '/UCHC/PublicShare/RNASeq_Workshop/RedSpruce/gfold/'. The job will create following files:

<pre style="color: silver; background: black;">-bash-4.2$ ls /UCHC/PublicShare/RNASeq_Workshop/RedSpruce/gfold/
cotreated_VS_ambient.diff	cotreated_VS_ambient.diff.ext 
cotreated_VS_elevated.diff	cotreated_VS_elevated.diff.ext
elevated_VS_ambient.diff	elevated_VS_ambient.diff.ext</pre>

The first few lines of the output file of cotreated_VS_ambient.diff will look like:

<pre style="color: silver; background: black;"># This file is generated by gfold V1.1.4 on Wed Jul 19 13:51:02 2017
# Normalization constants :
#    cotreated  52838582        1.0984
#    elevated   55466744        1
# The GFOLD value could be considered as a reliable log2 fold change.
# It is positive/negative if the gene is up/down regulated.
# A gene with zero GFOLD value should never be considered as 
# differentially expressed. For a comprehensive description of 
# GFOLD, please refer to the manual.
#GeneSymbol   GeneName   GFOLD(0.01)  E-FDR   log2fdc     1stRPKM   2ndRPKM
Gene.10065    Gene.10065   -0.15334      1     -0.2063     84.977    63.876
Gene.10096    Gene.10096    2.46607      1      2.5501     25.495   129.567
Gene.10097    Gene.10097    1.90102      1      2.072      27.710   101.209
Gene.10138    Gene.10138    1.38044      1      1.4319     55.704   130.366
Gene.10198    Gene.10198   -0.24752      1     -0.2928    121.876    86.279
.
.
.
Gene.10423   Gene.10423     2.83385      1      2.91202     26.908   175.725
Gene.10440   Gene.10440     0            1     -0.05898     55.542    46.239
Gene.10487   Gene.10487     2.39166      1      2.56462      7.236    37.198</pre>

The output file contains 7 columns, namely “GeneSymbol”, “GeneName”, “GFOLD(0.01)”,
“E-FDR”, “log2fdc”, “1stRPKM”, “2ndRPKM”. From that, we will focus on the GFOLD column for each gene. The GFOLD value, can be considered as a reliable log2 fold value indicating the change. The positive value indicate a up regulated and a negative value indicate a down regulated gene.

<h2 id = "Tenth_Point_Header">Final steps</h2>

While not included in this tutorial, the analysis is not quite finished yet. We still do not know what these genes are! Taking our differentially expressed genes we may annotate them through 'blast2go' or some other functional annotation software (like EnTap). While our organism may never be docile enough to be a model, we can at least contribute to its knowledge-base through our hard work and dedication in pipelines such as these.

Congratulations on finishing your tutorial!
